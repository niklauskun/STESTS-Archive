{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e18615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3adc2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Storage Setting\n",
    "Ts = 1/12\n",
    "c = 20\n",
    "eta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229e5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Training Performance\n",
    "base_path = 'models/BAW0EDH1MC20.0/Region{}/models_profit_julia_{}_100_lr001.csv'\n",
    "\n",
    "# Define your regions\n",
    "Regions = [1,2,3,4,5,6]\n",
    "\n",
    "# Initialize a dictionary to hold your DataFrames\n",
    "ESmodelTrain = {}\n",
    "\n",
    "# Loop over each region and read the corresponding CSV file\n",
    "for region in Regions:\n",
    "    # Format the file path with the current region\n",
    "    file_path = base_path.format(region, region)\n",
    "    \n",
    "    # Read the CSV file and save it into the dictionary\n",
    "    ESmodelTrain[region-1] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df44fd7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 180233, saw 79\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Bid Ahead\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# case = 'Strategic/UC25ED1_Strategic_true_Seg1_Load1.0_Fuel1.2_Error0.25_ratio1.0_MIP0.1_DARTDP_Hete'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# case = 'Strategic/BidAhead/UC25ED1_Strategic_true_Seg1_Load1.0_Fuel1.2_Error0.25_ratio1.0_MIP0.1_DARTDP_BAW36'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# case = 'Strategic/MarginalCost/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC10.0'\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# case = 'Strategic/EDH/UC25ED13_Strategic_false_ratio1.0_Seg1_BAW0_MC20.0_1'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m case \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrategic/2022/BS/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0_1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m ESD \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcase\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/EDESD.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ESC \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mcase\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/EDESC.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m price \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mcase\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/EDprice.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 180233, saw 79\n"
     ]
    }
   ],
   "source": [
    "### Bid Ahead\n",
    "# case = 'Strategic/UC25ED1_Strategic_true_Seg1_Load1.0_Fuel1.2_Error0.25_ratio1.0_MIP0.1_DARTDP_Hete'\n",
    "# case = 'Strategic/BidAhead/UC25ED1_Strategic_true_Seg1_Load1.0_Fuel1.2_Error0.25_ratio1.0_MIP0.1_DARTDP_BAW36'\n",
    "# case = 'Strategic/MarginalCost/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC10.0'\n",
    "# case = 'Strategic/EDH/UC25ED13_Strategic_false_ratio1.0_Seg1_BAW0_MC20.0_1'\n",
    "case = 'Strategic/2022/BS/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0_1'\n",
    "\n",
    "ESD = pd.read_csv('output/'+case+'/EDESD.csv',header=None)\n",
    "ESC = pd.read_csv('output/'+case+'/EDESC.csv',header=None)\n",
    "price = pd.read_csv('output/'+case+'/EDprice.csv',header=None)\n",
    "ES = pd.read_csv('2032 ADS PCM V2.4.1 Public Data/Processed Data/7Regions/Storage_C_4hr_5GW_Strategic.csv')\n",
    "ES = pd.read_csv('2032 ADS PCM V2.4.1 Public Data/Processed Data/2022/Storage_C_4hr_5GW_Strategic.csv')\n",
    "ESmodel = pd.read_csv('output/'+case+'/Strategic/storage_to_index_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1615155",
   "metadata": {},
   "outputs": [],
   "source": [
    "HeteroData = []\n",
    "for i in ESmodel['StorageID']:\n",
    "    index = i-1\n",
    "    Zone = ES['Zone'][index]\n",
    "    Capacity = ES['MaxCap(MWh)'][index]\n",
    "    PowerCap = ES['MaxCap(MW)'][index]\n",
    "    Revenue = sum((ESD[index]-ESC[index])*price[ES['Zone'][index]-1])/1000\n",
    "    Profit = sum((ESD[index]-ESC[index])*price[ES['Zone'][index]-1] - Ts*c*ESD[index])/1000\n",
    "    UnitProfit = Profit/Capacity\n",
    "#     ModelID = ESmodel[ESmodel['StorageID']==i]['SelectedModelIndex'].item()\n",
    "#     TrainProfit = ESmodelTrain[ES['Zone'][index]-1]['Training profit'][ModelID-1]/1000\n",
    "    \n",
    "    HeteroData.append({\n",
    "        'Storage ID': i,\n",
    "        'Zone': Zone,\n",
    "        'Capacity': Capacity,\n",
    "        'PowerCapacity': PowerCap,\n",
    "        'Revenue': Revenue,\n",
    "        'Profit': Profit,\n",
    "        'UnitProfit': UnitProfit,\n",
    "#         'ModelID': ModelID,\n",
    "#         'TrainProfit': TrainProfit\n",
    "    })\n",
    "HeteroDatadf = pd.DataFrame(HeteroData)\n",
    "Zone1ES = HeteroDatadf[HeteroDatadf['Zone']==1]\n",
    "Zone2ES = HeteroDatadf[HeteroDatadf['Zone']==2]\n",
    "Zone3ES = HeteroDatadf[HeteroDatadf['Zone']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74631a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zone1ES</th>\n",
       "      <th>Zone2ES</th>\n",
       "      <th>Zone3ES</th>\n",
       "      <th>Zone4ES</th>\n",
       "      <th>Zone5ES</th>\n",
       "      <th>Zone6ES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.718433</td>\n",
       "      <td>2.607461</td>\n",
       "      <td>2.82566</td>\n",
       "      <td>5.143952</td>\n",
       "      <td>2.971723</td>\n",
       "      <td>3.508194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Zone1ES   Zone2ES  Zone3ES   Zone4ES   Zone5ES   Zone6ES\n",
       "0  2.718433  2.607461  2.82566  5.143952  2.971723  3.508194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store results\n",
    "UnitProfit = {}\n",
    "\n",
    "# Calculate weighted average of UnitProfit based on Capacity for each Zone (1 through 6)\n",
    "for zone in range(1, 7):\n",
    "    zone_df = HeteroDatadf[HeteroDatadf['Zone'] == zone]\n",
    "    if not zone_df.empty:  # Check if the DataFrame is not empty\n",
    "        weighted_unit_profit = sum(zone_df['UnitProfit'] * zone_df['Capacity']) / sum(zone_df['Capacity'])\n",
    "    else:\n",
    "        weighted_unit_profit = None  # In case there's no data for the zone\n",
    "    UnitProfit[f'Zone{zone}ES'] = weighted_unit_profit\n",
    "\n",
    "# Convert the results dictionary to a single-row DataFrame\n",
    "UnitProfit_df = pd.DataFrame([UnitProfit])\n",
    "\n",
    "UnitProfit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0388d7d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TrainProfit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TrainProfit'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     subset \u001b[38;5;241m=\u001b[39m HeteroDatadf[HeteroDatadf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m zone]\n\u001b[1;32m     27\u001b[0m     colors \u001b[38;5;241m=\u001b[39m cmap(norm(subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPowerCapacity\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 28\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainProfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnitProfit\u001b[39m\u001b[38;5;124m'\u001b[39m], marker\u001b[38;5;241m=\u001b[39mzone_markers[zone], c\u001b[38;5;241m=\u001b[39mcolors, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Adding a color bar that reflects the full range of 'Capacity' values across all zones\u001b[39;00m\n\u001b[1;32m     31\u001b[0m sm \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mScalarMappable(cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/STESTS/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TrainProfit'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Temporary exlcude Zone 4\n",
    "HeteroDatadf = HeteroDatadf[HeteroDatadf['Zone']!=4]\n",
    "\n",
    "# Define a mapping of Zone to markers\n",
    "zone_markers = {\n",
    "    1.0: 'o',  # Circle\n",
    "    2.0: 's',  # Square\n",
    "    3.0: '^',  # Triangle up\n",
    "    4.0: '>',  # Triangle right\n",
    "    5.0: '<',  # Triangle left\n",
    "    6.0: 'p',  # Pentagon\n",
    "}\n",
    "\n",
    "# Assuming HeteroDatadf is already defined\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Normalize 'Capacity' for the entire dataset to get a consistent color mapping\n",
    "norm = mcolors.Normalize(vmin=HeteroDatadf['PowerCapacity'].min(), vmax=HeteroDatadf['PowerCapacity'].max())\n",
    "\n",
    "# Get the colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Plot points for each zone with respective markers, color normalized by 'Capacity'\n",
    "for zone in sorted(HeteroDatadf['Zone'].unique()):\n",
    "    subset = HeteroDatadf[HeteroDatadf['Zone'] == zone]\n",
    "    colors = cmap(norm(subset['PowerCapacity']))\n",
    "    plt.scatter(subset['TrainProfit'], subset['UnitProfit'], marker=zone_markers[zone], c=colors, alpha=0.5)\n",
    "\n",
    "# Adding a color bar that reflects the full range of 'Capacity' values across all zones\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, alpha=0.5)\n",
    "cbar.set_label('Capacity (MW)')\n",
    "\n",
    "# Annotations remain the same\n",
    "for i in range(len(HeteroDatadf)):\n",
    "    plt.text(HeteroDatadf['TrainProfit'].iloc[i], HeteroDatadf['UnitProfit'].iloc[i], f\" {HeteroDatadf['ModelID'].iloc[i]}\", fontsize=9, verticalalignment='bottom')\n",
    "\n",
    "# Adjust legend for zone markers\n",
    "legend_entries = [plt.Line2D([0], [0], color='black', marker=zone_markers[zone], linestyle='None', markersize=10, label=f'Zone {int(zone)}') for zone in sorted(HeteroDatadf['Zone'].unique())]\n",
    "plt.legend(handles=legend_entries, title=\"Zones\")\n",
    "\n",
    "plt.title('Train Profit vs. Unit Profit (100% AI ES) with Capacity Colors and Zone Markers')\n",
    "plt.xlabel('Train Profit ($/kWh)')\n",
    "plt.ylabel('Unit Profit ($/kWh)')\n",
    "plt.grid(True)\n",
    "plt.ylim([0, 17])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bcde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Using capacity as the color variable\n",
    "scatter = plt.scatter(HeteroDatadf['TrainProfit'], HeteroDatadf['UnitProfit'], marker='x', c=HeteroDatadf['PowerCapacity'], cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Adding a color bar to indicate the capacity values\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Capacity (MW)')\n",
    "\n",
    "# Annotating each point with its capacity number (without \"MWh\" unit in the annotation)\n",
    "for i in range(len(HeteroDatadf)):\n",
    "    plt.text(HeteroDatadf['TrainProfit'].iloc[i], HeteroDatadf['UnitProfit'].iloc[i], f\" {HeteroDatadf['ModelID'].iloc[i]}\", fontsize=9)\n",
    "\n",
    "# Adding a legend to explain the annotations\n",
    "plt.legend([scatter], ['ModelID'], loc=\"upper right\", title=\"Annotations\")\n",
    "\n",
    "plt.title('Train Profit vs. Unit Profit (100% AI ES) with Capacity Colors and Zone Markers')\n",
    "\n",
    "plt.xlabel('Train Profit ($/kWh)')\n",
    "plt.ylabel('Unit Profit ($/kWh)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Using capacity as the color variable\n",
    "scatter = plt.scatter(Zone2ES['TrainProfit'], Zone2ES['UnitProfit'], marker='x', c=Zone2ES['PowerCapacity'], cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Adding a color bar to indicate the capacity values\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Capacity (MW)')\n",
    "\n",
    "# Annotating each point with its capacity number (without \"MWh\" unit in the annotation)\n",
    "for i in range(len(Zone2ES)):\n",
    "    plt.text(Zone2ES['TrainProfit'].iloc[i], Zone2ES['UnitProfit'].iloc[i], f\" {Zone2ES['ModelID'].iloc[i]}\", fontsize=9)\n",
    "\n",
    "# Adding a legend to explain the annotations\n",
    "plt.legend([scatter], ['ModelID'], loc=\"upper right\", title=\"Annotations\")\n",
    "\n",
    "plt.title('Train Profit vs. Unit Profit (100% AI ES) with Capacity Colors')\n",
    "\n",
    "plt.xlabel('Train Profit ($/kWh)')\n",
    "plt.ylabel('Unit Profit ($/kWh)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67102c",
   "metadata": {},
   "source": [
    "### Benchmark 7 Regions Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e445cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Heterogeneous Storage Profit\n",
    "ESD = pd.read_csv('output/Strategic/MC20/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0/EDESD.csv',header=None)\n",
    "ESC = pd.read_csv('output/Strategic/MC20/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0/EDESC.csv',header=None)\n",
    "price = pd.read_csv('output/Strategic/MC20/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0/EDprice.csv',header=None)\n",
    "ES = pd.read_csv('2032 ADS PCM V2.4.1 Public Data/Processed Data/7Regions/Storage_C_4hr_5GW.csv')\n",
    "ESmodel = pd.read_csv('output/Strategic/MC20/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0/Strategic/storage_to_index_map.csv')\n",
    "# ESmodelTrain = pd.read_csv('models/models_profit_julia_2_100_lr001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "HeteroData = []\n",
    "for i in ESmodel['StorageID']:\n",
    "    index = i-1\n",
    "    Zone = ES['Zone'][index]\n",
    "    Capacity = ES['MaxCap(MWh)'][index]\n",
    "    PowerCap = ES['MaxCap(MW)'][index]\n",
    "    Revenue = sum((ESD[index]-ESC[index])*price[ES['Zone'][index]-1])/1000\n",
    "    Profit = sum((ESD[index]-ESC[index])*price[ES['Zone'][index]-1] - Ts*c*ESD[index])/1000\n",
    "    UnitProfit = Profit/Capacity\n",
    "    ModelID = ESmodel[ESmodel['StorageID']==i]['SelectedModelIndex'].item()\n",
    "    TrainProfit = ESmodelTrain[ES['Zone'][index]-1]['Training profit'][ModelID-1]/1000\n",
    "    \n",
    "    HeteroData.append({\n",
    "        'Storage ID': i,\n",
    "        'Zone': Zone,\n",
    "        'Capacity': Capacity,\n",
    "        'PowerCapacity': PowerCap,\n",
    "        'Revenue': Revenue,\n",
    "        'Profit': Profit,\n",
    "        'UnitProfit': UnitProfit,\n",
    "        'ModelID': ModelID,\n",
    "        'TrainProfit': TrainProfit\n",
    "    })\n",
    "HeteroDatadf = pd.DataFrame(HeteroData)\n",
    "Zone1ES = HeteroDatadf[HeteroDatadf['Zone']==1]\n",
    "Zone2ES = HeteroDatadf[HeteroDatadf['Zone']==2]\n",
    "Zone3ES = HeteroDatadf[HeteroDatadf['Zone']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e509c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store results\n",
    "UnitProfit = {}\n",
    "\n",
    "# Calculate weighted average of UnitProfit based on Capacity for each Zone (1 through 6)\n",
    "for zone in range(1, 7):\n",
    "    zone_df = HeteroDatadf[HeteroDatadf['Zone'] == zone]\n",
    "    if not zone_df.empty:  # Check if the DataFrame is not empty\n",
    "        weighted_unit_profit = sum(zone_df['UnitProfit'] * zone_df['Capacity']) / sum(zone_df['Capacity'])\n",
    "    else:\n",
    "        weighted_unit_profit = None  # In case there's no data for the zone\n",
    "    UnitProfit[f'Zone{zone}ES'] = weighted_unit_profit\n",
    "\n",
    "# Convert the results dictionary to a single-row DataFrame\n",
    "UnitProfit_df = pd.DataFrame([UnitProfit])\n",
    "\n",
    "UnitProfit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Temporary exlcude Zone 4\n",
    "HeteroDatadf = HeteroDatadf[HeteroDatadf['Zone']!=4]\n",
    "\n",
    "# Define a mapping of Zone to markers\n",
    "zone_markers = {\n",
    "    1.0: 'o',  # Circle\n",
    "    2.0: 's',  # Square\n",
    "    3.0: '^',  # Triangle up\n",
    "    4.0: '>',  # Triangle right\n",
    "    5.0: '<',  # Triangle left\n",
    "    6.0: 'p',  # Pentagon\n",
    "}\n",
    "\n",
    "# Assuming HeteroDatadf is already defined\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Normalize 'Capacity' for the entire dataset to get a consistent color mapping\n",
    "norm = mcolors.Normalize(vmin=HeteroDatadf['PowerCapacity'].min(), vmax=HeteroDatadf['PowerCapacity'].max())\n",
    "\n",
    "# Get the colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Plot points for each zone with respective markers, color normalized by 'Capacity'\n",
    "for zone in sorted(HeteroDatadf['Zone'].unique()):\n",
    "    subset = HeteroDatadf[HeteroDatadf['Zone'] == zone]\n",
    "    colors = cmap(norm(subset['PowerCapacity']))\n",
    "    plt.scatter(subset['TrainProfit'], subset['UnitProfit'], marker=zone_markers[zone], c=colors, alpha=0.5)\n",
    "\n",
    "# Adding a color bar that reflects the full range of 'Capacity' values across all zones\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, alpha=0.5)\n",
    "cbar.set_label('Capacity (MW)')\n",
    "\n",
    "# Annotations remain the same\n",
    "for i in range(len(HeteroDatadf)):\n",
    "    plt.text(HeteroDatadf['TrainProfit'].iloc[i], HeteroDatadf['UnitProfit'].iloc[i], f\" {HeteroDatadf['ModelID'].iloc[i]}\", fontsize=9, verticalalignment='bottom')\n",
    "\n",
    "# Adjust legend for zone markers\n",
    "legend_entries = [plt.Line2D([0], [0], color='black', marker=zone_markers[zone], linestyle='None', markersize=10, label=f'Zone {int(zone)}') for zone in sorted(HeteroDatadf['Zone'].unique())]\n",
    "plt.legend(handles=legend_entries, title=\"Zones\")\n",
    "\n",
    "plt.title('Train Profit vs. Unit Profit (100% AI ES) with Capacity Colors and Zone Markers')\n",
    "plt.xlabel('Train Profit ($/kWh)')\n",
    "plt.ylabel('Unit Profit ($/kWh)')\n",
    "plt.ylim([0, 17])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839136ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Using capacity as the color variable\n",
    "scatter = plt.scatter(Zone2ES['TrainProfit'], Zone2ES['UnitProfit'], marker='x', c=Zone2ES['PowerCapacity'], cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Adding a color bar to indicate the capacity values\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Capacity (MWh)')\n",
    "\n",
    "# Annotating each point with its capacity number (without \"MWh\" unit in the annotation)\n",
    "for i in range(len(Zone2ES)):\n",
    "    plt.text(Zone2ES['TrainProfit'].iloc[i], Zone2ES['UnitProfit'].iloc[i], f\" {Zone2ES['ModelID'].iloc[i]}\", fontsize=9)\n",
    "\n",
    "# Adding a legend to explain the annotations\n",
    "plt.legend([scatter], ['ModelID'], loc=\"upper right\", title=\"Annotations\")\n",
    "\n",
    "plt.title('Train Profit vs. Unit Profit (100% AI ES) with Capacity Colors')\n",
    "\n",
    "plt.xlabel('Train Profit ($/kWh)')\n",
    "plt.ylabel('Unit Profit ($/kWh)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Price Analysis\n",
    "# price = pd.read_csv('output/Strategic/Scale/UC25ED1_Strategic_true_Seg5_Load1.0_Fuel1.2_Error0.25_ratio0.5_MIP0.1_DARTDP/EDprice.csv',header=None)\n",
    "# df = pd.concat([price.mean()*12, (price*12).std()], axis=1)\n",
    "# df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emission\n",
    "ThermalGen = pd.read_csv('2032 ADS PCM V2.4.1 Public Data/Processed Data/ThermalGen_Full_C.csv')\n",
    "case = 'Strategic/MC20/UC25ED1_Strategic_true_ratio1.0_Seg1_BAW0_MC20.0'\n",
    "EDGen = pd.read_csv('output/'+case+'/EDGen.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_output_across_segments(interval_output, thermalgen_row):\n",
    "    \"\"\"\n",
    "    Distribute a single interval's output across the generator's operational segments.\n",
    "\n",
    "    :param interval_output: The total output for the generator in a 5-minute interval.\n",
    "    :param thermalgen_row: A pandas Series containing the generator's segment data.\n",
    "    :return: A list where each element represents the output allocated to a segment.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    segment_outputs = []  # Stores the output allocated to each segment\n",
    "    remaining_output = interval_output\n",
    "    \n",
    "    # Base segment output\n",
    "    if remaining_output > 0:\n",
    "        base_output = min(remaining_output, thermalgen_row['IOMinCap(MW)'])\n",
    "        segment_outputs.append(base_output)\n",
    "        remaining_output -= base_output\n",
    "    else:\n",
    "        segment_outputs.append(0)\n",
    "\n",
    "    # Incremental segments\n",
    "    for i in range(2, 7):  # Assuming up to 6 segments\n",
    "        inc_cap_key = f'IncCap{i}(MW)'\n",
    "        if inc_cap_key in thermalgen_row.index and thermalgen_row[inc_cap_key] > 0:\n",
    "            if remaining_output > 0:\n",
    "                segment_capacity = thermalgen_row[inc_cap_key]\n",
    "                segment_output = min(remaining_output, segment_capacity)\n",
    "                segment_outputs.append(segment_output)\n",
    "                remaining_output -= segment_output\n",
    "            else:\n",
    "                segment_outputs.append(0)\n",
    "        else:\n",
    "            break  # No more segments defined\n",
    "\n",
    "    # If there are less than 6 segments, fill the remaining with 0s\n",
    "    while len(segment_outputs) < 6:\n",
    "        segment_outputs.append(0)\n",
    "\n",
    "    return segment_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the interval outputs for each segment\n",
    "interval_segment_outputs = {gen_id: [] for gen_id in EDGen.columns}\n",
    "\n",
    "for gen_id in EDGen.columns:\n",
    "    thermalgen_row = ThermalGen.loc[gen_id]  # Adjust this line based on how you match generators\n",
    "    \n",
    "    for interval_output in EDGen[gen_id]:\n",
    "        segment_outputs = distribute_output_across_segments(interval_output, thermalgen_row)\n",
    "        interval_segment_outputs[gen_id].append(segment_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for the aggregated hourly segment outputs\n",
    "hourly_segment_outputs = {gen_id: pd.DataFrame(columns=[f'Segment{i}' for i in range(1, 7)]) for gen_id in EDGen.columns}\n",
    "\n",
    "for gen_id, outputs in interval_segment_outputs.items():\n",
    "    # Convert list of outputs to DataFrame for easier manipulation\n",
    "    outputs_df = pd.DataFrame(outputs, columns=[f'Segment{i}' for i in range(1, 7)])\n",
    "    \n",
    "    # Aggregate to hourly by summing every 12 rows (12 intervals per hour)\n",
    "    hourly_data = outputs_df.groupby(outputs_df.index // 12).sum()\n",
    "    \n",
    "    hourly_segment_outputs[gen_id] = hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emission_factor(fuel_name):\n",
    "    if fuel_name.startswith('Coal'):\n",
    "        return 95.92\n",
    "    elif fuel_name.startswith('NG'):\n",
    "        return 52.91\n",
    "    elif fuel_name.startswith('Oil'):\n",
    "        return 74.14\n",
    "    elif fuel_name == 'Petroleum Coke':\n",
    "        return 102.41\n",
    "    elif fuel_name == 'Geothermal' or fuel_name == 'Uranium':\n",
    "        return 0\n",
    "    elif fuel_name == 'Waste_Heat':\n",
    "        return 66.33\n",
    "    elif fuel_name in ['Bio_Landfill_Gas', 'Bio_Other']:\n",
    "        return 52.07\n",
    "    elif fuel_name == 'Bio_Wood':\n",
    "        return 93.80\n",
    "    elif fuel_name == 'Bio_Blk_Liquor':\n",
    "        return 73.84\n",
    "    elif fuel_name == 'Bio_Solid_Waste':\n",
    "        return 90.70\n",
    "    elif fuel_name == 'Bio_Agri_Res':\n",
    "        return 118.17\n",
    "    elif fuel_name == 'DefaultFuel':\n",
    "        return 52.91\n",
    "    else:\n",
    "        return None  # Or a default value if you have one for unknown types\n",
    "\n",
    "# Apply the function to create a new column\n",
    "ThermalGen['Emission Factor'] = ThermalGen['Fuel Name'].apply(map_emission_factor)\n",
    "\n",
    "# Check if there's any fuel type that did not get an emission factor (None values)\n",
    "missing_emission_factors = ThermalGen[ThermalGen['Emission Factor'].isnull()]\n",
    "if not missing_emission_factors.empty:\n",
    "    print(\"Missing emission factors for:\", missing_emission_factors['Fuel Name'].unique())\n",
    "else:\n",
    "    print(\"All fuel types have assigned emission factors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70f401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store hourly total emissions for all generators\n",
    "hourly_total_emissions = pd.DataFrame(index=hourly_segment_outputs[0].index)\n",
    "\n",
    "for gen_id in hourly_segment_outputs.keys():\n",
    "    thermalgen_row = ThermalGen.loc[gen_id]\n",
    "    hourly_data = hourly_segment_outputs[gen_id]\n",
    "    emission_factor = thermalgen_row['Emission Factor']\n",
    "    \n",
    "    # Initialize a Series to store total hourly fuel consumption for this generator\n",
    "    hourly_fuel_consumption = pd.Series(0, index=hourly_data.index)\n",
    "\n",
    "    # Calculate fuel consumption for each segment\n",
    "    for i in range(1, 7):  # Assuming up to 6 segments\n",
    "        segment_key = f'Segment{i}'\n",
    "        if i == 1:\n",
    "            # Base segment calculation\n",
    "            base_fuel = (hourly_data[segment_key] * thermalgen_row['MinInput(MMBTu)'] / thermalgen_row['IOMinCap(MW)']).fillna(0) / 12\n",
    "            hourly_fuel_consumption += base_fuel\n",
    "        else:\n",
    "            # Incremental segments\n",
    "            hr_key = f'IncHR{i}(MMBTu/MWh)'\n",
    "            if hr_key in thermalgen_row.index and thermalgen_row[hr_key] > 0:\n",
    "                incremental_fuel = (hourly_data[segment_key] * thermalgen_row[hr_key]).fillna(0) / 12\n",
    "                hourly_fuel_consumption += incremental_fuel\n",
    "\n",
    "    # Calculate total hourly emissions for this generator\n",
    "    hourly_emissions = hourly_fuel_consumption * emission_factor\n",
    "    hourly_total_emissions[gen_id] = hourly_emissions\n",
    "\n",
    "hourly_total_emissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdf5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum across rows and plot\n",
    "row_sums = hourly_total_emissions.sum(axis=1)/1000\n",
    "window_size = 24  # For example, for hourly data, a 24-hour (daily) moving average\n",
    "smoothed_sums = row_sums.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(row_sums.index, row_sums.values, label='Original', alpha=0.5)  # Making the original line semi-transparent\n",
    "plt.plot(smoothed_sums.index, smoothed_sums.values, label='Smoothed (Moving Average)', linewidth=2)\n",
    "\n",
    "plt.title('Hourly Total Emissions with Moving Average Smoothing (start up not added)')\n",
    "plt.xlabel('Hour')  # Adjust label if your index has a different meaning\n",
    "plt.ylabel('Total Emissions (ton)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
